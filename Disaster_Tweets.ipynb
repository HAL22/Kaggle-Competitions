{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyZ8g/LgIB/QmR0Ffld4HT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAL22/Kaggle-Competitions/blob/main/Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook for this kaggle competitions**: [disaster-tweets](https://www.kaggle.com/competitions/nlp-getting-started)\n",
        "\n",
        "\n",
        "*   Data can be found on the kaggle competition\n",
        "*   Different versions of this notebook are on kaggle\n",
        "\n",
        "\n",
        "**Best score of notebook**: 0.83512\n",
        "\n",
        "**Best rank**: 102/1167"
      ],
      "metadata": {
        "id": "dQd9UobUXHMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets transformers[sentencepiece]\n",
        "!apt install git-lfs\n",
        "!pip install evaluate\n",
        "!pip install -U huggingface_hub\n",
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "8jIbNvdhTWl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import AutoConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "Yhlj_DmoTXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"distilbert-base-uncased\"\n",
        "output_dir=\"disaster\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=2,\n",
        "weight_decay=0.01,\n",
        "evaluation_strategy=\"epoch\",\n",
        "save_strategy=\"epoch\",\n",
        "load_best_model_at_end=True,"
      ],
      "metadata": {
        "id": "FqDhYoBDTmZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
        "pd_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
        "\n",
        "pd_train.head()"
      ],
      "metadata": {
        "id": "M423CS4VTnTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates in training set\n",
        "duplicates_train = pd_train[pd_train[[\"text\"]].duplicated()]\n",
        "duplicates_train.head()"
      ],
      "metadata": {
        "id": "cjq9DrkwVwC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates in test set\n",
        "duplicates_test = pd_test[pd_test[[\"text\"]].duplicated()]\n",
        "duplicates_test.head()"
      ],
      "metadata": {
        "id": "Ip6rJseSV1fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows in training\n",
        "pd_train = pd_train.drop_duplicates(subset=[\"text\"],keep=False)\n",
        "pd_train.head()"
      ],
      "metadata": {
        "id": "xv3wp2zfV2L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_train.rename(columns = {\"target\":\"label\"}, inplace = True)"
      ],
      "metadata": {
        "id": "ApNnwnEQV73l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_train.head()"
      ],
      "metadata": {
        "id": "4fvkWYurWC2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "pd_train = pd_train[['text', 'label']].copy()\n",
        "df_train, df_test = train_test_split(pd_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "JeRLae7aWJU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = Dataset.from_pandas(df_train)\n",
        "test = Dataset.from_pandas(df_test)\n",
        "\n",
        "dataset = DatasetDict()\n",
        "dataset['train'] = train\n",
        "dataset['test'] = test"
      ],
      "metadata": {
        "id": "fA_IVoM9WKVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "metadata": {
        "id": "StYxHI7RWPgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "JiU8wvb7WQNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "F-DqybYPWSyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "QHKZ4g1GWVTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "k8jbe0AkWZjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "qmUlxgOMWgKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "qbyhlNBAWnZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "     output_dir= \"disaster\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=pretrained_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "BuEenuv9Wp6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SzQceG99WtcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_test"
      ],
      "metadata": {
        "id": "y1MNMScDWwEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\"disaster/checkpoint-1488\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"disaster/checkpoint-1488\")\n",
        "trained_model = AutoModelForSequenceClassification.from_pretrained(\"disaster/checkpoint-1488\")"
      ],
      "metadata": {
        "id": "SZGtc2SUWzAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = []\n",
        "\n",
        "pd_test = pd_test[['text']].copy()\n",
        "\n",
        "for tx in pd_test.text.tolist():\n",
        "    inps = tokenizer(tx, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = trained_model(**inps).logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    target.append(int(predicted_class_id))"
      ],
      "metadata": {
        "id": "x984qhyrW16f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_subm = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
        "pd_subm = pd_subm[['id']].copy()\n",
        "pd_subm['target'] = target"
      ],
      "metadata": {
        "id": "cAhK8AdeW-Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_subm.to_csv(\"/kaggle/working/submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "6MQRkSeqXBfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}